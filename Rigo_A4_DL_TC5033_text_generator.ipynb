{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreac941/tutorials/blob/main/Rigo_A4_DL_TC5033_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "### Team 1:\n",
        "- Alexis Hernández Martínez A01016308\n",
        "- Rigoberto Vega Escudero A01793132\n",
        "- Rodrigo Rodríguez Rodríguez A01183284\n",
        "- Andrea Carolina Treviño Garza A01034993\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries:\n",
        "import numpy as np\n",
        "# - PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# - Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# - Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# - For neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "# - Random number generation in a range\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d8ff971",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6d8ff971",
        "outputId": "8ddd6062-4549-44e1-8b76-ebee50eab1fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Validating if there's GPU available:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation required for splitting datasets\n",
        "# !pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZjGf2Ic__3C",
        "outputId": "5c97bf93-1caa-4b3b-9697-e566dbebbc99"
      },
      "id": "3ZjGf2Ic__3C",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "# Splitting the training, validation and testing datasets from WikiText2:\n",
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "# get_tokenizer from pytorch generates the tokens for a string sentence (divide each word with commas)\n",
        "tokeniser = get_tokenizer('basic_english')\n",
        "# yield_tokens function is defined to tokenize each sentence of the dataset\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# build_vocab_from_iterator from pytorch allows to build a vocabulary with a token iterator (like yield_tokens),\n",
        "# 'specials' allows to add symbols and preserved the order of tokens\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "# set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "# Variable to set the lenght of the data for function data_process.\n",
        "seq_length = 50\n",
        "# Function definition for processing each dataset considering a sequence lenght\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter] # tokenization is applied for each sentence in dataset\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) # Remove empty tensors,\n",
        "    # concatenating a tupple (ordered and unchangeable collection of data) with no empty strings from input data\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))  # Function returns the processed data in sentences of 50 tokens\n",
        "\n",
        "# Create tensors for each dataset (training, validation and testing) with the desired sentence lenght:\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "# Creating a dataset of tensors for training, validation and testing dataset\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # choose a batch size that fits your computation resources\n",
        "# Using DataLoader to iterate each tensor dataset in minibatches of previously defined batch_size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "59c63b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c63b01",
        "outputId": "0b10c396-ebbe-4c4c-ad61-5daa61fe13ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embeddings): Embedding(28785, 1024)\n",
              "  (lstm): LSTM(1024, 1024, num_layers=2, batch_first=True, dropout=0.65)\n",
              "  (fc): Linear(in_features=1024, out_features=28785, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__() # Initialization of class hierarchy, super is used to refer to parent class.\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size) # Lookup table that stores embeddings of a dictionary with specified size (embed_size)\n",
        "        self.hidden_size = hidden_size # Size definition of hidden layers\n",
        "        self.num_layers = num_layers # Definition of number of layers\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True,dropout=0.65) # Applying LSTM RNN to input sentences.\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size) # Applyng Linear (fully connected layer)\n",
        "\n",
        "    def forward(self, text, hidden): # Forward propagation definition\n",
        "        embeddings = self.embeddings(text) # Embeddings of text input\n",
        "        output, hidden = self.lstm(embeddings, hidden) # concatenation of the forward and reverse hidden state at each time step in the sequence\n",
        "        decoded = self.fc(output) # Definition of linear layer\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device), # Creating a 3D tensor filled with zeros of the specified dimensions\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)) # .to(device) allows to move tensor to the specified computing device\n",
        "\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 1024 # embedding size\n",
        "neurons = 1024 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 2 # the number of nn.LSTM layers\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eWIUkG4zm0ty"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimizer):\n",
        "    model = model.to(device) # to move our tensor model to the specified computing device\n",
        "    model.train() # Set the model to training mode (to activate training specific behaviors)\n",
        "    criterion = nn.CrossEntropyLoss() # CrossEntropy criterion selecting for training, combining softmax activiation and negative log.\n",
        "    # Computing the loss regarding the models predictions and true labels.\n",
        "\n",
        "    for epoch in range(epochs): # For loop for each training epoch.\n",
        "        total_loss = 0 # Initializing total_loss in zero\n",
        "        for data, targets in train_loader:\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Send the input data and targets to the computing device\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden = model.init_hidden(batch_size) # Calling init_hidden class to create a 3D tensor filled with zeros of the batch size\n",
        "            hidden = (hidden[0].to(device), hidden[1].to(device))  # Moving hidden tensor to device\n",
        "\n",
        "            # Forward pass\n",
        "            output, hidden = model(data, hidden) # concatenation of the forward and reverse hidden state of the model at each time step in the sequence\n",
        "\n",
        "            # Reshape output and targets for loss calculation\n",
        "            output = output.view(-1, vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(output, targets) # Using CrossEntropyLoss\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Aggregate loss for reporting\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f}') # printing for each epoch the average_loss computed"
      ],
      "id": "eWIUkG4zm0ty"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c84ce",
      "metadata": {
        "id": "aa9c84ce",
        "outputId": "b37d95e1-893b-4aab-984c-f5386e152885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - Loss: 6.9003\n",
            "Epoch 2/30 - Loss: 6.3710\n",
            "Epoch 3/30 - Loss: 6.1569\n",
            "Epoch 4/30 - Loss: 6.0091\n",
            "Epoch 5/30 - Loss: 5.8915\n",
            "Epoch 6/30 - Loss: 5.7906\n",
            "Epoch 7/30 - Loss: 5.7021\n",
            "Epoch 8/30 - Loss: 5.6223\n",
            "Epoch 9/30 - Loss: 5.5498\n",
            "Epoch 10/30 - Loss: 5.4821\n",
            "Epoch 11/30 - Loss: 5.4194\n",
            "Epoch 12/30 - Loss: 5.3611\n",
            "Epoch 13/30 - Loss: 5.3065\n",
            "Epoch 14/30 - Loss: 5.2544\n",
            "Epoch 15/30 - Loss: 5.2053\n",
            "Epoch 16/30 - Loss: 5.1580\n",
            "Epoch 17/30 - Loss: 5.1132\n",
            "Epoch 18/30 - Loss: 5.0700\n",
            "Epoch 19/30 - Loss: 5.0283\n",
            "Epoch 20/30 - Loss: 4.9869\n",
            "Epoch 21/30 - Loss: 4.9471\n",
            "Epoch 22/30 - Loss: 4.9078\n",
            "Epoch 23/30 - Loss: 4.8706\n",
            "Epoch 24/30 - Loss: 4.8327\n",
            "Epoch 25/30 - Loss: 4.7971\n",
            "Epoch 26/30 - Loss: 4.7619\n",
            "Epoch 27/30 - Loss: 4.7269\n",
            "Epoch 28/30 - Loss: 4.6920\n",
            "Epoch 29/30 - Loss: 4.6581\n",
            "Epoch 30/30 - Loss: 4.6250\n"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss() # Loss criterion definition\n",
        "lr = 0.00005 # Learning rate definition\n",
        "epochs = 30 # Number of epocs definition\n",
        "optimiser = optim.AdamW(model.parameters(), lr=lr) # Model optimizer definition\n",
        "train(model, epochs, optimiser) # Training the model with defined parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8667411",
      "metadata": {
        "id": "c8667411",
        "outputId": "783310bd-6e6c-4271-b6de-0a3c849347de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my family is the microphone of his own feet . lisa wants the confrontation in long – half a <unk> by a young dedication . harrison and the score of conventional children designed to raise a lot of a story , going from the verge of that he didn ' t tell it , but the phenomenon expressed the fuss from the covenant , as i and moore ' s worlds sign . death is the gentle contribution to the house of seacouver that he suggested . pointing , a few ingredients <unk> flowers on the plans into sydney which\n"
          ]
        }
      ],
      "source": [
        "# Now, let's implement the text generation function\n",
        "\n",
        "def generate_text(model, vocab, tokenizer, start_text, num_words, temperature=1.0, device='cuda'):\n",
        "    model.eval()  # Set mode to evaluation\n",
        "    words = tokenizer(start_text) # Tokenizing the input text\n",
        "    state_h, state_c = model.init_hidden(1) # Initializing hidden state\n",
        "\n",
        "    # Move the initial states to the device\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "\n",
        "    # Warm-up the hidden state by passing the seed words\n",
        "    for w in words: # for every word in every tokenized sentence\n",
        "        ix = torch.tensor([[vocab[w]]]).to(device) # Retrieves index for each word in vocabulary, used to create a Pytorch tensor\n",
        "        output, (state_h, state_c) = model(ix, (state_h, state_c)) # getting hidden states for capturing the context of previous steps\n",
        "        # and the output predictions of the model\n",
        "\n",
        "    # The new word is generated here after the warm-up\n",
        "    words.append(vocab.lookup_token(torch.argmax(output[0, -1]).item()))\n",
        "\n",
        "    # Generate the subsequent words\n",
        "    for _ in range(num_words - len(words)): # for loop meanwhile the amount of word is less than \"num_words\"\n",
        "        ix = torch.tensor([[vocab[words[-1]]]]).to(device) # Again - Retrieves index for each word in vocabulary, used to create a Pytorch tensor\n",
        "        with torch.no_grad():  # No need to track history in prediction mode\n",
        "            output, (state_h, state_c) = model(ix, (state_h, state_c)) # Again - getting hidden states for capturing the context of previous steps\n",
        "        # and the output predictions of the model\n",
        "\n",
        "        # Get the last word in the tensor of outputs, apply the temperature scaling\n",
        "        # and softmax to get probabilities\n",
        "        probabilities = F.softmax(output[0, -1] / temperature, dim=0).detach().cuda() # Probability definition for introducing controlled randomness\n",
        "        word_idx = torch.multinomial(probabilities, 1).item() # Generating a word index based on the probability distribution obtained from\n",
        "        # the output of the model\n",
        "\n",
        "        # Add the generated word to the sequence\n",
        "        words.append(vocab.lookup_token(word_idx))\n",
        "\n",
        "    return ' '.join(words) # Returning the words generated with '' spaces in between.\n",
        "\n",
        "# As with the train function, this code requires the model and related components to be defined.\n",
        "# The actual vocabulary object, tokenizer, and the model should be passed to this function when called.\n",
        "\n",
        "\n",
        "# Generate some text - 1:\n",
        "print(generate_text(model, vocab, tokeniser, start_text=\"My family is\", num_words=100, temperature=1.0, device='cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2884543",
      "metadata": {
        "id": "d2884543",
        "outputId": "a7241234-925b-4dbe-a8aa-8c791564427c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the planets in the solar system are the first several at basic mouth . the noisy miner had studied 0 mm ( 0 @ . @ 0 and this north ) , but serves the same edge as some of the guns had not had increased . lt main athletes buoys involve motorists and two rainfall us 250 <unk> can be used to more separate fire to a frame test . when the main firing tools is sandy rubbed that the damages begins to little narrow @-@ guns the distance covering the side of the main total carriage , and\n"
          ]
        }
      ],
      "source": [
        "# Generate some text - 2:\n",
        "print(generate_text(model, vocab, tokeniser, start_text=\"The planets in the solar system are\", num_words=100, temperature=1.0,  device='cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78eabe9e",
      "metadata": {
        "id": "78eabe9e",
        "outputId": "12d0bf6c-9de0-4662-b63f-2d25ee3e8418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thank you professor for the new concept , my plan is afraid to new true main ballad shran . supporting daniel of her choice from similar <unk> , anderson longing that the thing won blood and after five years ago , as king traditional countries , there caused it to be able to obtain the project . a video meant on a bonus version called a . irish media , authors primarily by husband ' s legacy directors and covetousness of the poetical with california gielgud and nobility also published the <unk> in 2005 . many of the search alphabets\n"
          ]
        }
      ],
      "source": [
        "# Generate some text - 3:\n",
        "print(generate_text(model, vocab, tokeniser, start_text=\"Thank you professor for\", num_words=100, temperature=1.0, device='cuda'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bibliography:**\n",
        "* LSTM — PyTorch 2.1 documentation. (n.d.). https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "* Anis, A. (2022, March 17). PyTorch LSTM: The Definitive Guide | cnvrg.io. cnvrg. https://cnvrg.io/pytorch-lstm/"
      ],
      "metadata": {
        "id": "7hOpJzrhjgWY"
      },
      "id": "7hOpJzrhjgWY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreac941/tutorials/blob/main/ProyectoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e65bcba-6200-4616-ba19-81257870d6e5",
      "metadata": {
        "id": "8e65bcba-6200-4616-ba19-81257870d6e5"
      },
      "source": [
        "## 1. Preprocess the Data\n",
        "First of all, we'll need to preprocess both the images and textcaptions.\n",
        "\n",
        "We use use a pre-trained model like ResNet for feature extraction to preprocess the images to match the input format expected by this model.\n",
        "\n",
        "For the captions preprocessing, we need to tokenize them, create a vocabulary and convert the captions to sequences of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca5f2ee-9155-40c0-8b01-4d1d14257fb1",
      "metadata": {
        "id": "0ca5f2ee-9155-40c0-8b01-4d1d14257fb1"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries:\n",
        "import torch # Import the PyTorch library\n",
        "import torch.nn as nn # Import the neural network module from PyTorch\n",
        "import torchvision.transforms as transforms # for data transformations.\n",
        "from PIL import Image # For image visualizarion\n",
        "from torch.nn.utils.rnn import pad_sequence # Used to pad a list of variable-length sequences to the same length (for RNN)\n",
        "from torch.utils.data import DataLoader, Dataset # For data processing\n",
        "from torchvision.models import resnet50 # For transfer learning\n",
        "import spacy  # For tokenization\n",
        "import os # To interact with operating system.\n",
        "import pandas as pd # To manage data manipulation\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        # Special tokens with fixed indices\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        # Reverse mapping: token to index\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "        # Frequency threshold for including words in the vocabulary\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the vocabulary\n",
        "        return len(self.itos)\n",
        "\n",
        "    def tokenizer_eng(self, text):\n",
        "        # Tokenize English text using Spacy\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        # Dictionary to store word frequencies\n",
        "        frequencies = {}\n",
        "        # Starting index for words in the vocabulary\n",
        "        idx = 4\n",
        "\n",
        "        # Iterate through each sentence in the input list\n",
        "        for sentence in sentence_list:\n",
        "            # Tokenize the sentence and iterate through each word\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                # Update word frequencies\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                # Check if the word frequency reaches the threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    # Add the word to the vocabulary\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        # Tokenize the input text\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        # Convert tokens to their corresponding indices or use <UNK> if not in vocabulary\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        # Store root directory, captions file path, and transformation function\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "\n",
        "        # Handling missing or non-string values in captions by filling NaN values with an empty string\n",
        "        self.df['caption'] = self.df['caption'].fillna('').astype(str)\n",
        "\n",
        "        # Store the transformation function for images\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get 'image' and 'caption' columns from the DataFrame\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize a vocabulary object with a specified frequency threshold\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "\n",
        "        # Build the vocabulary using the captions in the dataset\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    # rest of the class remains the same\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples in the dataset\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get caption and image ID for the given index\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "\n",
        "        # Load image from file using PIL and convert to RGB\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        # Apply the specified transformation to the image if available\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Numericalize the caption by converting it to a list of indices\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        # Return the image and its numericalized caption as a torch tensor\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        # Constructor to initialize the collate function with the specified padding index\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # __call__ method is called when an instance of the class is called as a function\n",
        "\n",
        "        # Extract images and targets from the batch\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0) # Concatenate images along the batch dimension\n",
        "        targets = [item[1] for item in batch]\n",
        "\n",
        "        # Pad the sequences in targets to create a batch\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        # Return the collated batch containing images and padded targets\n",
        "        return imgs, targets\n",
        "\n",
        "# Transformations for the image\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# DataLoader function\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    # Create a FlickrDataset instance with the specified parameters\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    # Get the padding index from the vocabulary of the dataset\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    # Create a DataLoader with the dataset and other specified parameters\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx), # Use the custom collate function\n",
        "    )\n",
        "\n",
        "    # Return the DataLoader and the vocabulary of the dataset\n",
        "    return loader, dataset.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f3385b-3d50-4de3-b4b1-210135bedda7",
      "metadata": {
        "id": "16f3385b-3d50-4de3-b4b1-210135bedda7"
      },
      "outputs": [],
      "source": [
        "# Use Spacy for tokenization - Load the English language model from SpaCy\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Set the root folder and annotation file paths\n",
        "root_folder='flickr30k_images/flickr30k_images'\n",
        "annotation_file='flickr30k_images/results.csv'\n",
        "#annotation_file = annotation_file['caption'].astype(str)\n",
        "\n",
        "# Initialize the data loader and get the vocabulary\n",
        "data_loader, vocab = get_loader(root_folder, annotation_file, transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a27fb6f-30b6-4bb3-9778-0e58c6cc4e0e",
      "metadata": {
        "id": "0a27fb6f-30b6-4bb3-9778-0e58c6cc4e0e"
      },
      "source": [
        "## 2. Build the Model\n",
        "We will create a CNN-RNN model. The CNN part will be a pre-trained ResNet model (without the classification head) for feature extraction, and the RNN part will be an LSTM network for generating captions.\n",
        "\n",
        "In this code:\n",
        "\n",
        "EncoderCNN uses a pre-trained ResNet50 model for image feature extraction.\n",
        "\n",
        "DecoderRNN is an LSTM network for generating captions.\n",
        "\n",
        "CNNtoRNN combines both the encoder and decoder.work for generating captions.\n",
        "CNNtoRNN combines both the encoder and decoder.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d13efc6-6fbc-4d4b-a084-fb9eef74fca5",
      "metadata": {
        "id": "7d13efc6-6fbc-4d4b-a084-fb9eef74fca5"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        \"\"\"\n",
        "        EncoderCNN initializes a CNN-based image encoder.\n",
        "\n",
        "        Args:\n",
        "        - embed_size (int): Size of the output embedding.\n",
        "        - train_CNN (bool): Flag indicating whether to train the CNN layers.\n",
        "        \"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "        # Load pre-trained ResNet-50 model from torchvision\n",
        "        self.inception = resnet50(pretrained=True)\n",
        "\n",
        "        # Replace the final fully connected layer to match the desired embed_size\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "\n",
        "        # Additional layers for non-linearity and dropout\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward pass of the EncoderCNN.\n",
        "\n",
        "        Args:\n",
        "        - images (torch.Tensor): Input images.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output embedding.\n",
        "        \"\"\"\n",
        "        # Pass images through the modified ResNet-50 model\n",
        "        features = self.inception(images)\n",
        "\n",
        "        # Don't backpropagate through the entire network/layers if not training CNN\n",
        "        if not self.train_CNN:\n",
        "            for param in self.inception.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Apply dropout and ReLU activation\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        \"\"\"\n",
        "        DecoderRNN initializes an RNN-based caption decoder.\n",
        "\n",
        "        Args:\n",
        "        - embed_size (int): Size of the input embedding.\n",
        "        - hidden_size (int): Size of the hidden state in the LSTM.\n",
        "        - vocab_size (int): Size of the vocabulary.\n",
        "        - num_layers (int): Number of layers in the LSTM.\n",
        "        \"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \"\"\"\n",
        "        Forward pass of the DecoderRNN.\n",
        "\n",
        "        Args:\n",
        "        - features (torch.Tensor): Image features.\n",
        "        - captions (torch.Tensor): Input captions.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output scores.\n",
        "        \"\"\"\n",
        "        # Embed input captions\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "\n",
        "        # Concatenate image features with embedded captions\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "\n",
        "        # Pass through LSTM and linear layer\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        \"\"\"\n",
        "        CNNtoRNN initializes a model combining EncoderCNN and DecoderRNN.\n",
        "\n",
        "        Args:\n",
        "        - embed_size (int): Size of the input/output embedding.\n",
        "        - hidden_size (int): Size of the hidden state in the LSTM.\n",
        "        - vocab_size (int): Size of the vocabulary.\n",
        "        - num_layers (int): Number of layers in the LSTM.\n",
        "        \"\"\"\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        # Initialize EncoderCNN and DecoderRNN\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        \"\"\"\n",
        "        Forward pass of the CNNtoRNN model.\n",
        "\n",
        "        Args:\n",
        "        - images (torch.Tensor): Input images.\n",
        "        - captions (torch.Tensor): Input captions.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output scores.\n",
        "        \"\"\"\n",
        "        # Pass images through EncoderCNN\n",
        "        features = self.encoderCNN(images)\n",
        "\n",
        "        # Pass features and captions through DecoderRNN\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        \"\"\"\n",
        "        Generate captions for images using the trained model.\n",
        "\n",
        "        Args:\n",
        "        - image (torch.Tensor): Input image.\n",
        "        - vocabulary (Vocabulary): Vocabulary object.\n",
        "        - max_length (int): Maximum length of generated captions.\n",
        "\n",
        "        Returns:\n",
        "        - List[str]: Generated caption as a list of words.\n",
        "        \"\"\"\n",
        "        result_caption = []\n",
        "\n",
        "        # Disable gradient computation during inference\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            # Generate captions word by word\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                # Break if \"<EOS>\" token is predicted\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        # Convert indices to words using vocabulary\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54d08c1-0521-4263-a5d0-8c64abf6b330",
      "metadata": {
        "id": "e54d08c1-0521-4263-a5d0-8c64abf6b330"
      },
      "source": [
        "## 3. Training\n",
        "Now, let's set up the training loop. In this training function, we calculate the loss for each batch and update the model's weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9962dd4-77d3-445f-9c60-fd38db0c965f",
      "metadata": {
        "id": "d9962dd4-77d3-445f-9c60-fd38db0c965f"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, vocab_size, device):\n",
        "    \"\"\"\n",
        "    Train the given model using the provided data loader, optimizer, and criterion.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The neural network model to be trained.\n",
        "    - data_loader (DataLoader): DataLoader providing training data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
        "    - criterion (nn.Module): The loss function used for training.\n",
        "    - vocab_size (int): Size of the vocabulary.\n",
        "    - device (torch.device): Device (CPU or GPU) on which to perform training.\n",
        "\n",
        "    Returns:\n",
        "    - float: Average training loss over the entire dataset.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    # Initialize the total loss\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate through batches in the data loader\n",
        "    for idx, (images, captions) in enumerate(data_loader):\n",
        "        # Move images and captions to the specified device\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Zero the gradients in the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions[:-1])\n",
        "        # Reshape outputs and captions for calculating the loss\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions.reshape(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate and return the average training loss\n",
        "    return total_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc2f2f51-7111-42fa-92d6-324afbf59cde",
      "metadata": {
        "id": "fc2f2f51-7111-42fa-92d6-324afbf59cde"
      },
      "source": [
        "## Step 4: Evaluation with BLEU\n",
        "For evaluating an image captioning model, you can use metrics like BLEU (Bilingual Evaluation Understudy Score). However, please note that BLEU is not perfect and might not always align with human judgment. It's typically used to get a quantitative estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48bfeca7-623c-42a4-925c-f58dbda7c7f1",
      "metadata": {
        "id": "48bfeca7-623c-42a4-925c-f58dbda7c7f1"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the given model using the provided data loader and vocabulary.\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): The neural network model to be evaluated.\n",
        "    - data_loader (DataLoader): DataLoader providing evaluation data.\n",
        "    - device (torch.device): Device (CPU or GPU) on which to perform evaluation.\n",
        "    - vocab (Vocabulary): Vocabulary object for converting indices to words.\n",
        "\n",
        "    Returns:\n",
        "    - float: BLEU-4 score calculated on the evaluation dataset.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Lists to store references and hypotheses for BLEU calculation\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    # Disable gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        # Iterate through batches in the data loader\n",
        "        for images, captions in data_loader:\n",
        "            # Move images and captions to the specified device\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Generate captions using the trained model\n",
        "            outputs = model.caption_image(images, vocab)\n",
        "            outputs = [[vocab.itos[idx] for idx in seq] for seq in outputs]\n",
        "\n",
        "            # Convert target captions to words\n",
        "            targets = [[vocab.itos[idx] for idx in seq] for seq in captions.cpu().numpy()]\n",
        "\n",
        "            # Collect references and hypotheses for BLEU calculation\n",
        "            references.extend(targets)\n",
        "            hypotheses.extend(outputs)\n",
        "\n",
        "    # Calculate BLEU-4 score using NLTK's corpus_bleu function\n",
        "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    # Return the BLEU-4 score\n",
        "    return bleu4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810a8b91-e647-477b-9914-909f49c7ed96",
      "metadata": {
        "id": "810a8b91-e647-477b-9914-909f49c7ed96"
      },
      "source": [
        "## 5. Train the Data\n",
        "\n",
        "Finally, we initialized the model, the optimizer, and the loss function and start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e9c02d-f47c-4dcb-b06b-d08398f1855c",
      "metadata": {
        "id": "26e9c02d-f47c-4dcb-b06b-d08398f1855c",
        "outputId": "74781ad7-71cd-45f1-85f7-87b019a50dc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\AHERNANDEZ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\AHERNANDEZ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\AHERNANDEZ/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
            "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:08<00:00, 12.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters definition\n",
        "embed_size = 256  # Size of the word embeddings\n",
        "hidden_size = 512  # Size of the hidden state in the LSTM\n",
        "vocab_size = len(vocab)  # Size of the vocabulary\n",
        "num_layers = 1  # Number of layers in the LSTM\n",
        "learning_rate = 3e-4  # Learning rate for the optimizer\n",
        "\n",
        "# Initialize the data loader and get the vocabulary\n",
        "data_loader, vocab = get_loader(root_folder, annotation_file, transform)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Choose device (GPU if available, else CPU)\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)  # Create the CNN-to-RNN model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])  # Cross-entropy loss with padding ignored\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training and evaluation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate through batches in the data loader\n",
        "    for idx, (images, captions) in enumerate(data_loader):\n",
        "        images, captions = images.to(device), captions.to(device)  # Move data to the specified device\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions[:-1])  # Predict captions (excluding the last token)\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions.reshape(-1))  # Calculate the loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        loss.backward()  # Perform backward pass\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        total_loss += loss.item()  # Accumulate the total loss\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)  # Calculate average loss\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    bleu4 = evaluate(model, data_loader, device, vocab)  # Evaluate BLEU-4 score\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], BLEU-4: {bleu4:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d3a82b-2678-4fb1-ae4a-7c59be86a30f",
      "metadata": {
        "id": "c2d3a82b-2678-4fb1-ae4a-7c59be86a30f"
      },
      "source": [
        "## 6. Generate a Prediction\n",
        "You need a function to transform a new input image into the same format as the training images, using the caption_image method from the CNNtoRNN class. Here's a function that takes an image path and generates a caption:\n",
        "\n",
        "In this code, generate_caption takes the path of the image you want to caption, uses the trained model directly, and processes the image using the same transform as during training. The model variable is assumed to be the same one you trained earlier in the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96802c5c-1f30-4f8d-a317-e58d665aa11a",
      "metadata": {
        "id": "96802c5c-1f30-4f8d-a317-e58d665aa11a"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path, transform=None):\n",
        "    \"\"\"\n",
        "    Load an image from the specified file path and apply optional transformations.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): File path of the image.\n",
        "    - transform (callable, optional): Transformation to be applied to the image.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Processed image tensor.\n",
        "    \"\"\"\n",
        "    # Open the image using PIL and convert it to RGB format\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply the specified transformation if provided\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)  # Add an extra dimension for batch (unsqueeze)\n",
        "\n",
        "    # Return the processed image tensor\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f724b3a-444e-468f-9fac-7ea1625e8bfe",
      "metadata": {
        "id": "7f724b3a-444e-468f-9fac-7ea1625e8bfe"
      },
      "source": [
        "## Make sure to replace 'path_to_your_image.jpg' with the path to your actual image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195f785d-6d18-49a7-80da-e74408afe2ce",
      "metadata": {
        "id": "195f785d-6d18-49a7-80da-e74408afe2ce"
      },
      "outputs": [],
      "source": [
        "# Function to generate captions for an input image\n",
        "def generate_caption(image_path, model, vocabulary, transform, device):\n",
        "    \"\"\"\n",
        "    Generate a caption for the input image using the trained model and vocabulary.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): File path of the input image.\n",
        "    - model (nn.Module): Trained model for generating captions.\n",
        "    - vocabulary (Vocabulary): Vocabulary object for converting indices to words.\n",
        "    - transform (callable): Transformation to be applied to the input image.\n",
        "    - device (torch.device): Device (CPU or GPU) on which to perform inference.\n",
        "\n",
        "    Returns:\n",
        "    - str: Generated caption for the input image.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Load and transform the input image\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        caption = model.caption_image(image, vocabulary, max_length=50)\n",
        "\n",
        "    # Filter out special tokens and join the words to form the final caption\n",
        "    caption = [word for word in caption if word not in {\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"}]\n",
        "    return ' '.join(caption)\n",
        "\n",
        "# Assume that `model` is already trained and available in your script\n",
        "\n",
        "# Define your transformations, same as used for the training images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to your input image\n",
        "image_path = 'path_to_your_image.jpg'\n",
        "\n",
        "# Generate caption for the input image\n",
        "caption = generate_caption(image_path, model, vocab, transform, device)\n",
        "print(\"Generated Caption:\", caption)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
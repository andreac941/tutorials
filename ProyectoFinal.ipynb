{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreac941/tutorials/blob/main/ProyectoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e65bcba-6200-4616-ba19-81257870d6e5",
      "metadata": {
        "id": "8e65bcba-6200-4616-ba19-81257870d6e5"
      },
      "source": [
        "## 1. Preprocess the Data\n",
        "First of all, we'll need to preprocess both the images and textcaptions.\n",
        "\n",
        "We use use a pre-trained model like ResNet for feature extraction to preprocess the images to match the input format expected by this model.\n",
        "\n",
        "For the captions preprocessing, we need to tokenize them, create a vocabulary and convert the captions to sequences of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca5f2ee-9155-40c0-8b01-4d1d14257fb1",
      "metadata": {
        "id": "0ca5f2ee-9155-40c0-8b01-4d1d14257fb1"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries:\n",
        "import torch\n",
        "import torchvision.transforms as transforms # for data transformations.\n",
        "from PIL import Image # For image visualizarion\n",
        "from torch.nn.utils.rnn import pad_sequence # Used to pad a list of variable-length sequences to the same length (for RNN)\n",
        "from torch.utils.data import DataLoader, Dataset # For data processing\n",
        "from torchvision.models import resnet50 # For transfer learning\n",
        "import spacy  # For tokenization\n",
        "import os # To interact with operating system.\n",
        "import pandas as pd # To manage data manipulation\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        # Special tokens with fixed indices\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        # Reverse mapping: token to index\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "        # Frequency threshold for including words in the vocabulary\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the vocabulary\n",
        "        return len(self.itos)\n",
        "\n",
        "    def tokenizer_eng(self, text):\n",
        "        # Tokenize English text using Spacy\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        # Dictionary to store word frequencies\n",
        "        frequencies = {}\n",
        "        # Starting index for words in the vocabulary\n",
        "        idx = 4\n",
        "\n",
        "        # Iterate through each sentence in the input list\n",
        "        for sentence in sentence_list:\n",
        "            # Tokenize the sentence and iterate through each word\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                # Update word frequencies\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                # Check if the word frequency reaches the threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    # Add the word to the vocabulary\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        # Tokenize the input text\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        # Convert tokens to their corresponding indices or use <UNK> if not in vocabulary\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "\n",
        "        # Handling missing or non-string values in captions\n",
        "        self.df['caption'] = self.df['caption'].fillna('').astype(str)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    # rest of the class remains the same\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "# Transformations for the image\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# DataLoader\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset.vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f3385b-3d50-4de3-b4b1-210135bedda7",
      "metadata": {
        "id": "16f3385b-3d50-4de3-b4b1-210135bedda7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Use Spacy for tokenization\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize the data loader and get the vocabulary\n",
        "root_folder='flickr30k_images/flickr30k_images'\n",
        "annotation_file='flickr30k_images/results.csv'\n",
        "#annotation_file = annotation_file['caption'].astype(str)\n",
        "\n",
        "data_loader, vocab = get_loader(root_folder, annotation_file, transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a27fb6f-30b6-4bb3-9778-0e58c6cc4e0e",
      "metadata": {
        "id": "0a27fb6f-30b6-4bb3-9778-0e58c6cc4e0e"
      },
      "source": [
        "## 2. Build the Model\n",
        "We will create a CNN-RNN model. The CNN part will be a pre-trained ResNet model (without the classification head) for feature extraction, and the RNN part will be an LSTM network for generating captions.\n",
        "\n",
        "In this code:\n",
        "\n",
        "EncoderCNN uses a pre-trained ResNet50 model for image feature extraction.\n",
        "\n",
        "DecoderRNN is an LSTM network for generating captions.\n",
        "\n",
        "CNNtoRNN combines both the encoder and decoder.work for generating captions.\n",
        "CNNtoRNN combines both the encoder and decoder.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d13efc6-6fbc-4d4b-a084-fb9eef74fca5",
      "metadata": {
        "id": "7d13efc6-6fbc-4d4b-a084-fb9eef74fca5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.inception = resnet50(pretrained=True)\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "\n",
        "        # Don't backpropagate through the entire network if not training CNN\n",
        "        if not self.train_CNN:\n",
        "            for param in self.inception.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54d08c1-0521-4263-a5d0-8c64abf6b330",
      "metadata": {
        "id": "e54d08c1-0521-4263-a5d0-8c64abf6b330"
      },
      "source": [
        "## 3. Training\n",
        "Now, let's set up the training loo. In this training function, we calculate the loss for each batch and update the model's weights.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9962dd4-77d3-445f-9c60-fd38db0c965f",
      "metadata": {
        "id": "d9962dd4-77d3-445f-9c60-fd38db0c965f"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, vocab_size, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for idx, (images, captions) in enumerate(data_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions[:-1])\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions.reshape(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc2f2f51-7111-42fa-92d6-324afbf59cde",
      "metadata": {
        "id": "fc2f2f51-7111-42fa-92d6-324afbf59cde"
      },
      "source": [
        "## Step 4: Evaluation with BLEU\n",
        "For evaluating an image captioning model, you can use metrics like BLEU (Bilingual Evaluation Understudy Score). However, please note that BLEU is not perfect and might not always align with human judgment. It's typically used to get a quantitative estimate of the model's performance. Here's an example of how you can implement a simple evaluation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48bfeca7-623c-42a4-925c-f58dbda7c7f1",
      "metadata": {
        "id": "48bfeca7-623c-42a4-925c-f58dbda7c7f1"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions in data_loader:\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Generate captions\n",
        "            outputs = model.caption_image(images, vocab)\n",
        "            outputs = [[vocab.itos[idx] for idx in seq] for seq in outputs]\n",
        "\n",
        "            # Convert targets to words\n",
        "            targets = [[vocab.itos[idx] for idx in seq] for seq in captions.cpu().numpy()]\n",
        "\n",
        "            # Collect references and hypotheses for BLEU calculation\n",
        "            references.extend(targets)\n",
        "            hypotheses.extend(outputs)\n",
        "\n",
        "    # Calculate BLEU-4 score\n",
        "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    return bleu4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810a8b91-e647-477b-9914-909f49c7ed96",
      "metadata": {
        "id": "810a8b91-e647-477b-9914-909f49c7ed96"
      },
      "source": [
        "## 5. Train the Data\n",
        "\n",
        "Finally,we  initialize the model, optimizer, and loss function, and start the training process.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e9c02d-f47c-4dcb-b06b-d08398f1855c",
      "metadata": {
        "id": "26e9c02d-f47c-4dcb-b06b-d08398f1855c",
        "outputId": "74781ad7-71cd-45f1-85f7-87b019a50dc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\AHERNANDEZ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\AHERNANDEZ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\AHERNANDEZ/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
            "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:08<00:00, 12.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = len(vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 3e-4\n",
        "\n",
        "# Initialize the data loader and get the vocabulary\n",
        "data_loader, vocab = get_loader(root_folder, annotation_file, transform)\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training and evaluation loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for idx, (images, captions) in enumerate(data_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions[:-1])\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), captions.reshape(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    bleu4 = evaluate(model, data_loader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], BLEU-4: {bleu4:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2d3a82b-2678-4fb1-ae4a-7c59be86a30f",
      "metadata": {
        "id": "c2d3a82b-2678-4fb1-ae4a-7c59be86a30f"
      },
      "source": [
        "## 6. Generate a Prediction\n",
        "You need a function to transform a new input image into the same format as the training images. You'll use the caption_image method from the CNNtoRNN class. Let's write a function to take an image path and generate a caption:\n",
        "\n",
        "In this code, generate_caption takes the path of the image you want to caption, uses the trained model directly, and processes the image using the same transform as during training. The model variable is assumed to be the same one you trained earlier in the script.\n",
        "s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96802c5c-1f30-4f8d-a317-e58d665aa11a",
      "metadata": {
        "id": "96802c5c-1f30-4f8d-a317-e58d665aa11a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f724b3a-444e-468f-9fac-7ea1625e8bfe",
      "metadata": {
        "id": "7f724b3a-444e-468f-9fac-7ea1625e8bfe"
      },
      "source": [
        "## Make sure to replace 'path_to_your_image.jpg' with the path to your actual image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195f785d-6d18-49a7-80da-e74408afe2ce",
      "metadata": {
        "id": "195f785d-6d18-49a7-80da-e74408afe2ce"
      },
      "outputs": [],
      "source": [
        "def generate_caption(image_path, model, vocabulary, transform, device):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        caption = model.caption_image(image, vocabulary, max_length=50)\n",
        "\n",
        "    # Convert caption tokens to words and join\n",
        "    caption = [word for word in caption if word not in {\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"}]\n",
        "    return ' '.join(caption)\n",
        "\n",
        "# Assume that `model` is already trained and available in your script\n",
        "\n",
        "# Define your transformations, same as used for the training images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function for generating captions\n",
        "def generate_caption(image_path, model, vocabulary, transform, device):\n",
        "    model.eval()\n",
        "    image = load_image(image_path, transform)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        caption = model.caption_image(image, vocabulary, max_length=50)\n",
        "\n",
        "    # Convert caption tokens to words and join\n",
        "    caption = [word for word in caption if word not in {\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\"}]\n",
        "    return ' '.join(caption)\n",
        "\n",
        "# Path to your input image\n",
        "image_path = 'path_to_your_image.jpg'\n",
        "\n",
        "# Generate caption\n",
        "caption = generate_caption(image_path, model, vocab, transform, device)\n",
        "print(\"Generated Caption:\", caption)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}